{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020-05-09  Ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aFbtD4uKrv3W",
        "t88oG-qc-16q",
        "qO3dY4c56Gfw",
        "bJWmUSDlAyOK",
        "YLx__wuPieBe",
        "0AvQgkfqSKEC",
        "TeHCabICQgtA",
        "konh6so3QisB",
        "fMGaKcfzRH7M",
        "RHJVdAb03sh_",
        "r_S9Z4by3seb",
        "bxyf2bX3aaep",
        "vEw5OQDyXvzl",
        "P2Hj6aBncgJk",
        "N8Gae7f3VB5Y",
        "OEBuuhlkwIMu",
        "ZjjuHB0rwULR",
        "6PdHdvEww3aD",
        "XiDidYG-jsQy",
        "NXUWwMTJw9dq",
        "qvOxz3SYw9h8",
        "6zK5cpuckTJ3",
        "Y1XBnMuCw9l6",
        "Kyosm9OZw9qZ",
        "AFcp77hOw9uI",
        "G5PI_pRfw91l",
        "SwHw2i3oyho5",
        "kgZbjX5iw-AF",
        "iqB656qNygjH",
        "-EK-RY0opSNX",
        "tjMcvT33ymnI",
        "NY5w7x_6yqab",
        "OXWSQuGTyqlO",
        "CrvIqWElqhzM",
        "5Kt66CjqyzCz",
        "b_0h1642y2nI",
        "wGGUGYR9y4LI",
        "DvZ5Q-fky4TV",
        "3p0Lc8RFy4XJ",
        "Q4GE559ly4Zw",
        "5ynxIB0azGk2",
        "WuuADgPUzGz6"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENYso35hnbpA",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# ┗(•̀へ •́ )ﾉ Hyun's Machine Learning (Ensemble)\n",
        "---\n",
        "자 Ensemble을 공부해봅시다 (´∇｀)  \n",
        "## Authored by. Hyun\n",
        "![미니언즈!](https://post-phinf.pstatic.net/MjAxNzA3MTBfMjIg/MDAxNDk5NjcxOTY1NDQw.Kz07JXiZg6AT6Y4PAZY7ubUNAr7rbDinLwFGuS0OOxcg.WVhpo8yfybUh0qImMGNAo1ucSUPuNOvQyzlO_vKlAlkg.JPEG/%EC%98%81%EC%A7%84%EC%9C%845.jpg?type=w1200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vC9LhV8N4VBB"
      },
      "source": [
        "# 목차"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xU8VOXp54VAs"
      },
      "source": [
        "## 1. 차차 정리해봅시다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw8bfsbHw98e",
        "colab_type": "text"
      },
      "source": [
        "# Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwHw2i3oyho5",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5fqVfqyt4jW",
        "colab_type": "text"
      },
      "source": [
        "#### Random Forest란?\n",
        "- Decision tree를 모아놓은 ensemble 모델\n",
        "- Random forest는 수많은 Decision tree가 모여 만들어진 숲이라고 생각하면 됨!\n",
        "- 많은 decision tree의 의견을 다수결로 최종 결과를 반환\n",
        "\n",
        "### 학습원리 \n",
        " 1. 주어진 train data에서 무작위로 중복을 허용해 n개를 선택\n",
        " 2. 선택한 n개의 data에서 데이터 feature를 중복 허용없이 d개를 선택\n",
        " 3. 이를 바탕으로 decision tree로 학습\n",
        " 4. 1~3을 k번 반복\n",
        " 5. k개의 decision tree로 평균값 or 다수결로 최종 예측값을 결정\n",
        "\n",
        " ### 메소드 모음\n",
        " - **RandomForestClassifier ( n_estimators = 100 , criterion = ' gini ' , max_depth = None , min_samples_split = 2 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0 , max_features = ' auto ' , max_leaf_nodes = None , min_impurity_decrease = 0 , min_impurity_split = None , bootstrap = True, oob_score = False , n_jobs = None, random_state = None , verbose = 0 , warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)** \n",
        "  - n_esitmators: 생성할 Decision Tree 개수\n",
        "  - n_jobs: 학습을 수행하기 위한 병렬적으로 활용할 CPU 코어 개수\n",
        "  - criterion: 정보 순도를 계산하기 위한 방법들. entropy, gini 중에 설정 가능\n",
        "  - oob_score: 예측이 얼마나 정확한지에 대한 추정을 수치로 나타낸 것"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqMMN3LqEm9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 예시 1\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics    \n",
        "# 학습 진행\n",
        "forest = RandomForestClassifier(n_estimators=100)\n",
        "forest.fit(x_train, y_train)\n",
        " \n",
        "# 예측\n",
        "y_pred = forest.predict(x_test)\n",
        "print(y_pred)\n",
        "print(list(y_test))\n",
        " \n",
        "# 정확도 확인\n",
        "print('정확도 :', metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW2Gc5zYE4cP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 예시 2\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "predicted = rf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predicted)\n",
        "\n",
        "print(f'Out-of-bag score estimate: {rf.oob_score_:.3}')\n",
        "print(f'Mean accuracy score: {accuracy:.3}')\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = pd.DataFrame(confusion_matrix(y_test, predicted), columns=iris.target_names, index=iris.target_names)\n",
        "sns.heatmap(cm, annot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgZbjX5iw-AF",
        "colab_type": "text"
      },
      "source": [
        "### Bagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMPjIuD2GIpI",
        "colab_type": "text"
      },
      "source": [
        "#### Bagging 이란?\n",
        "- Bootstrap Aggregating의 줄임말\n",
        "- 먼저 원래 데이터에 대해서 여러 개의 작은 데이터셋 N개를 샘플링해서 만든 다음, 각각의 데이터를 작은 모델 N개로 학습을 시킨다.\n",
        "- 그 다음 학습된 N개의 모델을 모두 하나로 합쳐서 최종적인 모델로 사용한다.\n",
        "- 병렬적으로 데이터를 나누어 여러 개의 모델을 동시에 학습시킴\n",
        "- 예시) Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqB656qNygjH",
        "colab_type": "text"
      },
      "source": [
        "### Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o2Pttc0F22A",
        "colab_type": "text"
      },
      "source": [
        "> #### Boosting이란?  \n",
        "![123123](https://quantdare.com/wp-content/uploads/2016/04/bb3.png)\n",
        "- Bagging은 샘플을 여러 번 뽑아 각 모델을 학습시켜 결과를 집계(Aggregating) 하는 방법이다.\n",
        "- Boosting은 Bagging과 다르게 이전 모델의 오차를 고려하여 학습을 하는 Ensemble 모델이다!\n",
        "- 못 맞춘 데이터에 대해서만 순차적으로 학습시켜서 여러개의 모델을 학습함. 그래서 bagging에 비해서 상대적으로 속도가 많이 느리다. 그 전 모델의 학습이 끝나야 다음 모델학습이 가능하기 때문."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbK5_CuiZtyx",
        "colab_type": "text"
      },
      "source": [
        "#### Gradient Boosting이란???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfhLo9TPZxvU",
        "colab_type": "text"
      },
      "source": [
        "- Gradient Boosting은 residual fitting이다!\n",
        "- Gradient Boosting은 최적의 파라미터 탐색 과정이(Gradient Descent) 함수 공간에서 이루어진다. 그래서 loss function을 파라미터가 현재까지 학습된 모델 함수로 미분한다.\n",
        "- 이때 미분되는 기울기는 Negative gradient인데 pseudo-residual이라고도 불리며, 이것은 어떤 데이터 포인트에서 loss function이 줄어들기 위해 f(x)가 가려고하는 방향이다.\n",
        "- 이 방향에 새로운 모델을 fitting해서 이것을 이전 모델과 결합하면, f(x) 는 loss function이 줄어드는 방향으로 업데이트한다!\n",
        "$$f_{i+1} = f_{i} - \\rho \\frac{\\partial{J}}{\\partial{f_{i}}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flscTqiHZxgl",
        "colab_type": "text"
      },
      "source": [
        "- ![](https://t1.daumcdn.net/cfile/tistory/99A9FC375C46C0201B)\n",
        "- 위 그림을 보시면 tree 1을 통해 예측하고 남은 잔차를 tree2를 통해 예측하고, 이를 반복함으로서 점점 잔차를 줄여나가는 것을 볼 수 있다. 이 때, 각각의 모델 tree1,2,3 을 weak learner, 이를 결합한 분류기를 strong learner라고 한다.\n",
        "- 보통 약한 분류기로는 간단한 의사결정나무 (decision tree)를 많이 사용합니다. 이를 Gradient boosting tree라고도 한다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EK-RY0opSNX",
        "colab_type": "text"
      },
      "source": [
        "#### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uouKdNy2n-8_",
        "colab_type": "text"
      },
      "source": [
        "- 사실 대부분 boosting 모델들이 parameter tuning을 하는 이유는 tree의 다형성과 overfitting 문제를 해결하기 위함이다.\n",
        "- Catboost 는 기본 파라미터가 기본적으로 최적화가 잘 되어있어서 parameter tuning에 크게 신경쓰지 않아도 됨\n",
        "- 반면 Xgboost나 LightGBM은 parameter tuning에 매우 민감하다.\n",
        "- Catboost 는 이를 내부적인 알고리즘으로 해결하고 있어서 parameter tuning을 할 필요가 없는 것이다. 굳이 한다면 learning_rate, random_strength, L2_regulariser 과 같은 parameter tuning인데, 결과는 큰 차이가 없다고 한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EQ8ppMDpvHh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "- [참고 사이트](http://machinelearningkorea.com/2019/09/29/lightgbm-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0/)\n",
        " <br>\n",
        "- max_depth[default=6]\n",
        " - 트리의 최대 깊이를 정의하는 parameter(DT에서 배웠음)\n",
        "- min_child_weight[default=1]\n",
        " - Overfitting을 컨트롤하는 파라미터로, 값이 높아지면, underfitting되는 경우가 있어 CV를 통해 튜닝되어야 함.\n",
        "- gamma[default=0]\n",
        " - 노드가 split되게 위한 loss function의 값이 감소하는 최소 값을 정의함. 값이 높아질수록 알고리즘은 보수적으로 변하고, loss function의 정의에 따라 적정값이 달라져 **반드시 튜닝되어야 한다**\n",
        "- subsample\n",
        " - 각 트리마다의 관측 데이터 샘플링 비율. 값을 적게 주면 over-fitting을 방지하지만 값을 너무 작게 주면 under-fitting(일반적으로 0.5~1).\n",
        "- colsample_bytree\n",
        " - 각 트리마다의 feature 샘플링 비율.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNg2pZuEqOeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## best parameter 찾는 예시 (밑에 GridsearchCV에도 같은 코드가 있음)\n",
        "# 우선 부차적인 parameter 찾아주고\n",
        "cb = CatBoostClassifier(\n",
        " learning_rate =0.1,\n",
        " iterations=100 #n-estimator대신 iteration을 사용,\n",
        ")\n",
        "\n",
        "cb_params_1 = {\n",
        "    'depth' : [3,5,7],\n",
        "    'random_strength' : [1,3],\n",
        "    'bagging_temperature' : [0,0.5,1],\n",
        "    'l2_leaf_reg' : [1,3,5,7],\n",
        "}\n",
        "cb_grid_1 = GridSearchCV(cb, param_grid=cb_params_1, scoring=my_scorer, cv=5, verbose=1)\n",
        "cb_grid_1.fit(train[features], train['Survived'])\n",
        "\n",
        "print(\"Best Score : {}\".format(cb_grid_1.best_score_))\n",
        "print(\"Best Params : {}\".format(cb_grid_1.best_params_))\n",
        "best_cb_model = cb_grid_1.best_estimator_\n",
        "\n",
        "# 최적의 core parameter 찾기\n",
        "cb_params_2 = {\n",
        "    'learning_rate' : [0.03, 0.07, 0.1],\n",
        "    'iterations' : [n for n in range(80,130,20)]\n",
        "}\n",
        "cb_grid_2 = GridSearchCV(best_cb_model, param_grid=cb_params_2, scoring=my_scorer, cv=5, verbose=1)\n",
        "cb_grid_2.fit(train[features], train['Survived'])\n",
        "\n",
        "print(\"Best Score : {}\".format(cb_grid_2.best_score_))\n",
        "print(\"Best Params : {}\".format(cb_grid_2.best_params_))\n",
        "best_cb_model = cb_grid_2.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjMcvT33ymnI",
        "colab_type": "text"
      },
      "source": [
        "#### CatBoost\n",
        "- Catboost는 Category와 Boosting을 합쳐서 만들어진 이름이다. \n",
        "- 여기에서 Boost는 Gradient boosting machine learnin algorithm에서 온 말인데 Gradient boosting은 추천 시스템, 예측 등 다양한 분야에서 활용되어지는 강력한 방법.\n",
        "- Deep Learning과 달리 적은 데이터로도 좋은 결과를 얻을 수 있는 효율적인 방법이다.\n",
        "- Catboost에서는 categorical 변수를 사용자가 다른 작업을 하지 않아도 자동으로 이를 변환하여 사용한다. model fitting할 때 지정해야됨!!!\n",
        "- default parameters값으로 더 나은 성능\n",
        "- hyper-parmeter tuning을 하지 않더라도 기본적인 세팅으로도 좋은 결과를 얻을 수 있어 활용성이 뛰어나다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bun_kQGCo9uQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CatBoost Regressor 예시\n",
        "from catboost import CatBoostRegressor\n",
        "train_data = pd.read_csv('train_data.csv', encdoing='euc-kr')\n",
        "test_data = pd.read_csv('test_data.csv', encdoing='euc-kr')\n",
        "cat_features = [0,1,2]\n",
        "train_labels = [10,20,30]\n",
        "\n",
        "model = CatBoostRegressor(iterations=2, learning_rate=1, depth=2)\n",
        "\n",
        "model.fit(train_data, train_labels, cat_features) # cat_features를 꼭 지정해줘야한다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXRmy-mQo_9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CatBoost Classifier 예시\n",
        "import catboost as cb\n",
        "cat_features_index = [0,1,2,3,4,5,6]\n",
        "\n",
        "def auc(m, train, test): \n",
        "    return (metrics.roc_auc_score(y_train,m.predict_proba(train)[:,1]),\n",
        "                            metrics.roc_auc_score(y_test,m.predict_proba(test)[:,1]))\n",
        "\n",
        "params = {'depth': [4, 7, 10],\n",
        "          'learning_rate' : [0.03, 0.1, 0.15],\n",
        "         'l2_leaf_reg': [1,4,9],\n",
        "         'iterations': [300]}\n",
        "cb = cb.CatBoostClassifier()\n",
        "cb_model = GridSearchCV(cb, params, scoring=\"roc_auc\", cv = 3)\n",
        "cb_model.fit(train, y_train)\n",
        "\n",
        "# With Categorical features\n",
        "clf = cb.CatBoostClassifier(eval_metric=\"AUC\", depth=10, iterations= 500, l2_leaf_reg= 9, learning_rate= 0.15)\n",
        "clf.fit(train,y_train)\n",
        "auc(clf, train, test)\n",
        "\n",
        "# With Categorical features\n",
        "clf = cb.CatBoostClassifier(eval_metric=\"AUC\",one_hot_max_size=31, \\\n",
        "                            depth=10, iterations= 500, l2_leaf_reg= 9, learning_rate= 0.15)\n",
        "clf.fit(train,y_train, cat_features= cat_features_index)\n",
        "auc(clf, train, test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY5w7x_6yqab",
        "colab_type": "text"
      },
      "source": [
        "#### Xgboost\n",
        "[Xgboost Hyperparameter](https://xgboost.readthedocs.io/en/latest/parameter.html)\n",
        "- XGBoost는 Gradient Boosting tree 알고리즘에서 유명하고 효율적인 오픈 소스 구현\n",
        "- Gradient Boosting은 더욱 단순하고 약한 모델 세트의 추정치의 ensemble을 결합하여 대상 변수를 정확하게 예측하려 시도하는 지도 학습 알고리즘\n",
        "- Null값 있어도 상관없는 모델!\n",
        "- categorical variables는 모두 one-hot encoding을 통해서 바꿔줘야 함!\n",
        "- `tree_method = 'gpu_hist'` hyperparameter에 이거 넣으면 계산속도 빨라짐!!!\n",
        "-  `n_jobs`: Number of parallel threads used to run xgboost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTtYgRztokxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Xgboost Classifier\n",
        "from xgboost import XGBClassifier, plot_importance\n",
        "\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "model.predict(X_test)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_score(y_test,y_pred, average='weighted')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5F9hpbeop4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Xgboost Plot importance\n",
        "# 코드1\n",
        "from xgboost import plot_importance\n",
        "\n",
        "model = xgb.XGBClassifier()\n",
        "model.fit(X, y)\n",
        "sorted_idx = np.argsort(model.feature_importances_)[::-1]\n",
        "for index in sorted_idx:\n",
        "    print([X.columns[index], model.feature_importances_[index]])\n",
        "\n",
        "# 코드2\n",
        "from xgboost import XGBClassifier, plot_importance\n",
        "model = XGBClassifier()\n",
        "model.fit(train, label)\n",
        "\n",
        "sorted_idx = np.argsort(model.feature_importances_)[::-1]\n",
        "\n",
        "for index in sorted_idx:\n",
        "    print([train.columns[index], model.feature_importances_[index]]) \n",
        "\n",
        "    plot_importance(model, max_num_features = 15)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXWSQuGTyqlO",
        "colab_type": "text"
      },
      "source": [
        "#### LightGBM\n",
        "- Tree based learning 알고리즘의 gradient boosting framework!\n",
        "- 작은 데이터셋 규모로 진행했을 때 overfitting될 가능성이 매우 높기 때문에 10,000 row 이상인 데이터셋에 적합하다.\n",
        "- Catboost와 마찬가지로 categorical \n",
        "- 일반적인 GBM 패키지와 다르게 LGBM은 leaf wise방식을 사용한다. 기존의 트리기반 알고리즘은 트리의 깊이를 줄이기 위해 level wise(균형 트리 분할)를 사용한다면 LGBM은 leaf wise(리프 중심 트리 분할)를 이용한다. 앞의 level wise는 트리를 균형적으로 분할하는데 균형작업이 추가로 들어간다고 보면 된다. LGBM은 균형적으로 트리를 분할하지 않는 대신 최대 손실값(max delta loss)을 갖는 트리 노드를 계속 분할한다. 이 때문에 비대칭적으로 어느 트리는 깊이가 아주 깊어지게 된다. 이 방식은 균형 트리 분할보다 오류를 최소화할 수 있다.\n",
        "![](https://user-images.githubusercontent.com/46089347/67144522-3479aa80-f2b2-11e9-9a17-1206005286f6.png)\n",
        "\n",
        "> 주요 hyper parameter\n",
        "- n_estimators : default=100, 반복하려는 트리의 갯수\n",
        "learning)rate : 0~1사이 값 지정. gradient descent에서 얼마나 움직일 것인지 설정한다. 간단히 학습률이라고 생각하면 된다.\n",
        "- max_depth : default=-1, 최대 깊이를 조절\n",
        "- min_child_samples : default=20, leaf node로 분류되기 위한 최소 데이터 수\n",
        "- num_leaves : default=31, one tree가 가잘 수 있는 leaf 갯수\n",
        "- boost : default=gbdt, gbdt는 gradient descent를 의미. rt는 random forest\n",
        "- reg_lambda : L2 규제 적용\n",
        "- leg_alpha : L1 규제\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn-FHxW0qdj1",
        "colab_type": "text"
      },
      "source": [
        "기본적인 lightgbm 쓰는 방법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7QjBAlPqc2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn import metrics\n",
        "\n",
        "def auc2(m, train, test): \n",
        "    return (metrics.roc_auc_score(y_train,m.predict(train)),\n",
        "                            metrics.roc_auc_score(y_test,m.predict(test)))\n",
        "\n",
        "lg = lgb.LGBMClassifier(silent=False)\n",
        "param_dist = {\"max_depth\": [25,50, 75],\n",
        "              \"learning_rate\" : [0.01,0.05,0.1],\n",
        "              \"num_leaves\": [300,900,1200],\n",
        "              \"n_estimators\": [200]\n",
        "             }\n",
        "grid_search = GridSearchCV(lg, n_jobs=-1, param_grid=param_dist, cv = 3, scoring=\"roc_auc\", verbose=5)\n",
        "grid_search.fit(train,y_train)\n",
        "grid_search.best_estimator_\n",
        "\n",
        "d_train = lgb.Dataset(train, label=y_train)\n",
        "params = {\"max_depth\": 50, \"learning_rate\" : 0.1, \"num_leaves\": 900,  \"n_estimators\": 300}\n",
        "\n",
        "# Without Categorical Features\n",
        "model2 = lgb.train(params, d_train)\n",
        "auc2(model2, train, test)\n",
        "\n",
        "#With Catgeorical Features\n",
        "cate_features_name = [\"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\"AIRLINE\",\"DESTINATION_AIRPORT\",\n",
        "                 \"ORIGIN_AIRPORT\"]\n",
        "model2 = lgb.train(params, d_train, categorical_feature = cate_features_name)\n",
        "auc2(model2, train, test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrvIqWElqhzM",
        "colab_type": "text"
      },
      "source": [
        "##### Kaggle_card_fraud_detection에서 썼던 코드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvetBrzeqwSe",
        "colab_type": "text"
      },
      "source": [
        "Google Drive 마운트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngNYjA_oqlqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LNwFrfKqlvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd /content/gdrive/My \\Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U4B5Aj7qlzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 사용자에 맞게 바꾸기\n",
        "# cd Colab \\Notebooks/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwz6n8Neql3P",
        "colab_type": "text"
      },
      "source": [
        "Google Colab 사용 시 설치법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg6Dx8llq3Yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone --recursive https://github.com/Microsoft/LightGBM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9Qpoodmq3dX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd LightGBM/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfRFrlYMq3hk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir build"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K88iB4L-q3lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cmake -DUSE_GPU=1 #avoid ..\n",
        "!make -j$(nproc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbbgrNHBq3qH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo apt-get -y install python-pip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqFs4VCwq3ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo -H pip install setuptools pandas numpy scipy scikit-learn -U"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kky8ZN9rq36g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd python-package/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpM7se7iq4Dw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo python setup.py install --precompile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH-iFODEq4QG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lgbm 설치시 pandas에서 오류가 나 재설정한 부분(선택적)\n",
        "!pip install pandas==0.18.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjeBPFMOq4Zp",
        "colab_type": "text"
      },
      "source": [
        "LGBM 설치된 경로로 추가 설치과정 필요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXn1WmcLq4fZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd /content/gdrive/My \\Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aspwch-Nq4XN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 사용자에 따른 선택적, 개별 드라이브 상황에 따라 경로 설정-> but LightGBM을 설치한 경로에 맞춰야 함\n",
        "cd 경로/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txuqad5Xq4Mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd LightGBM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIlNMtMVq4Jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd python-package"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBxQZ340q4Au",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo python setup.py install --precompile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4Hvrlwsq33J",
        "colab_type": "text"
      },
      "source": [
        "분석을 위한 경로 재설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf1oQbkBrEWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lG-l4YirEff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd /content/gdrive/My \\Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOa1oUE0rEkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 사용자에 따른 선택적, 개별 드라이브 상황에 따라 경로 설정-> but LightGBM을 설치한 경로에 맞춰야 함\n",
        "cd 경로"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvDUnDimrEol",
        "colab_type": "text"
      },
      "source": [
        "기본 패키지 및 설정, 데이터 적재"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3KSbDCKrEsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSJXHLktrExZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('final_train_merged.csv')    # 맞는 데이터 적재하기\n",
        "test = pd.read_csv('final_test_merged.csv')    # 맞는 데이터 적재하기"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WErXw01frE1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test에 고객 아이디가 없어서 아이디 있는 파일 중 아무거나 가져옴. \n",
        "# test용 고객 아이디 있는 데이터면 수정해도 무방\n",
        "tt = pd.read_csv('pca_test_all_c_fraud.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hvi4MMEyq3z8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del train['addr1_na']\n",
        "del test['addr1_na']\n",
        "\n",
        "del train['Unnamed: 0']\n",
        "del test['Unnamed: 0']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhiGTioHrMU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = train.iloc[:, 1:].values\n",
        "y = train.iloc[:, 0].values\n",
        "\n",
        "X_test = test.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTdamFplrMbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X.shape, y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79ZZmRQ_rMj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6B1M9RvrMoK",
        "colab_type": "text"
      },
      "source": [
        "LGBM 모델 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXJ3aQfcrMrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import PredefinedSplit\n",
        "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
        "from sklearn.metrics import (roc_curve, auc, accuracy_score)\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBiuqzdDrMwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {\n",
        "    'task': 'train',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'metric': {'binary_logloss', 'auc'},\n",
        "    'metric_freq': 1,\n",
        "    'is_training_metric': True,\n",
        "    'max_bin': 255,\n",
        "    'learning_rate': 0.01,\n",
        "    'num_leaves': 63,\n",
        "    'tree_learner': 'serial',\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'min_data_in_leaf': 50,\n",
        "    'min_sum_hessian_in_leaf': 5,\n",
        "    'is_enable_sparse': True,\n",
        "    'use_two_round_loading': False,\n",
        "    'is_save_binary_file': False,\n",
        "    'output_model': 'LightGBM_model.txt',\n",
        "    'num_machines': 1,\n",
        "    'local_listen_port': 12400,\n",
        "    'machine_list_file': 'mlist.txt',\n",
        "    'verbose': 0,\n",
        "    'subsample_for_bin': 200000,\n",
        "    'min_child_samples': 20,\n",
        "    'min_child_weight': 0.001,\n",
        "    'min_split_gain': 0.0,\n",
        "    'colsample_bytree': 1.0,\n",
        "    'reg_alpha': 0.0,\n",
        "    'reg_lambda': 0.0\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4ajsl13rM0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgb_train = lgb.Dataset(X_train, y_train)\n",
        "lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgqIB8ZFrM5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = lgb.train(params, lgb_train, 2500, lgb_eval, verbose_eval=10,  early_stopping_rounds=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzxObaWgrM84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_valid_pred = model.predict(X_valid)\n",
        "roc_auc_score(y_valid, y_valid_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7kMm2crrMhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVmXelhErTme",
        "colab_type": "text"
      },
      "source": [
        "제출용 csv 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA5-ZDj-rTsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tt['isFraud'] = prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeRyCvCErTwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tt2 = tt[['TransactionID', 'isFraud']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RQwYkT2rT0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tt2.to_csv('lgbm_git_.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}